## 对抗攻击算法



![img](https://pic3.zhimg.com/80/v2-80c67557bb2e6ed370b82bd706f7cafe_720w.webp)



对抗攻击算法是一类恶意的技术手段，旨在欺骗或迷惑机器学习系统、深度神经网络或其他模型。这些算法通过对输入数据进行精心设计的修改，使其在外观上与原始数据相似，但能够使模型产生错误的输出结果。

对抗攻击算法可以分为两大类：

1. 白盒攻击：黑盒攻击和白盒攻击是对抗攻击的两个常见的角度。白盒攻击是指攻击者完全了解目标模型的内部结构和参数，并能够访问模型的训练数据和梯度信息。在白盒攻击中，攻击者可以使用更多的信息和技巧来生成对抗样本。

2. 黑盒攻击：黑盒攻击是指攻击者只能通过对模型的输入和输出进行观察来推断模型的行为，无法直接访问模型的内部信息。在黑盒攻击中，攻击者可能只能根据有限的信息进行对抗样本的生成，这增加了攻击的难度，但仍然可以在很大程度上影响模型的输出。

常见的对抗攻击算法包括：

- FGSM（Fast Gradient Sign Method）: 一种基于梯度的快速攻击方法。

```python
outputs = self.net(advimage)
loss = loss_adv(self.loss, outputs, labels, target_labels, self.target, self.device)

updatas = torch.autograd.grad(loss, [advimage])[0].detach()  #通过loss计算得到扰动数据
if self.p == np.inf:
    updatas = updatas.sign()
else:
    normval = torch.norm(updatas.view(batchsize, -1), self.p, 1)
    updatas = updatas / normval.view(batchsize, 1, 1, 1)

advimage = advimage + updatas*self.eps  #添加扰动 updatas为扰动数据 eps为扰动幅度

#处理扰动 进行限制等操作
delta = advimage - images
if self.p==np.inf:
    delta = torch.clamp(delta, -self.eps, self.eps)
else:
    normVal = torch.norm(delta.view(batchsize, -1), self.p, 1)
    mask = normVal<=self.eps
    scaling = self.eps/normVal
    scaling[mask] = 1
    delta = delta*scaling.view(batchsize, 1, 1, 1)    


advimage = images+delta
advimage = torch.clamp(advimage, 0, 1)

```

- PGD（Projected Gradient Descent）: 一种迭代的对抗攻击方法，通过多次迭代地应用 FGSM 来生成对抗样本。



```
outputs = self.net(advimage)
loss = loss_adv(self.loss, outputs, labels, target_labels, self.target, self.device)

updatas = torch.autograd.grad(loss, [advimage])[0].detach()  #通过loss计算得到扰动数据
if self.p == np.inf:
    updatas = updatas.sign()
else:
    normval = torch.norm(updatas.view(batchsize, -1), self.p, 1)
    updatas = updatas / normval.view(batchsize, 1, 1, 1)

advimage = advimage + updatas*self.eps  #添加扰动 updatas为扰动数据 eps为扰动幅度

#处理扰动 进行限制等操作
delta = advimage - images
if self.p==np.inf:
    delta = torch.clamp(delta, -self.eps, self.eps)
else:
    normVal = torch.norm(delta.view(batchsize, -1), self.p, 1)
    mask = normVal<=self.eps
    scaling = self.eps/normVal
    scaling[mask] = 1
    delta = delta*scaling.view(batchsize, 1, 1, 1)    


advimage = images+delta
advimage = torch.clamp(advimage, 0, 1)

```



- DeepFool：一种通过线性逼近的方式寻找最小扰动量的攻击方法。



```
outputs = self.net(advimage)
loss = loss_adv(self.loss, outputs, labels, target_labels, self.target, self.device)

updatas = torch.autograd.grad(loss, [advimage])[0].detach()  #通过loss计算得到扰动数据
if self.p == np.inf:
    updatas = updatas.sign()
else:
    normval = torch.norm(updatas.view(batchsize, -1), self.p, 1)
    updatas = updatas / normval.view(batchsize, 1, 1, 1)

advimage = advimage + updatas*self.eps  #添加扰动 updatas为扰动数据 eps为扰动幅度

#处理扰动 进行限制等操作
delta = advimage - images
if self.p==np.inf:
    delta = torch.clamp(delta, -self.eps, self.eps)
else:
    normVal = torch.norm(delta.view(batchsize, -1), self.p, 1)
    mask = normVal<=self.eps
    scaling = self.eps/normVal
    scaling[mask] = 1
    delta = delta*scaling.view(batchsize, 1, 1, 1)    


advimage = images+delta
advimage = torch.clamp(advimage, 0, 1)

```



- DeepFool：一种通过线性逼近的方式寻找最小扰动量的攻击方法。

```python
delta = torch.rand_like(image)*2*self.epsilon-self.epsilon
if self.p!=np.inf: 
    normVal = torch.norm(delta.view(batchsize, -1), self.p, 1)#求范数
    mask = normVal<=self.epsilon
    scaling = self.epsilon/normVal
    scaling[mask] = 1
    delta = delta*scaling.view(batchsize, 1, 1, 1)
advimage = image+delta  #初始扰动图片


for i in range(self.steps):  #pgd的迭代次数
    #进行FGSM操作
    advimage = advimage.clone().detach().requires_grad_(True) # 
    
    netOut = self.net(advimage)
    
    loss = loss_adv(self.loss, netOut, label, target_labels, self.target, self.device)        
    updates = torch.autograd.grad(loss, [advimage])[0].detach()
    if self.p==np.inf:
        updates = updates.sign()
    else:
        normVal = torch.norm(updates.view(batchsize, -1), self.p, 1)
        updates = updates/normVal.view(batchsize, 1, 1, 1)
    updates = updates*self.stepsize
    advimage = advimage+updates
    # project the disturbed image to feasible set if needed
    delta = advimage-image
    if self.p==np.inf:
        delta = torch.clamp(delta, -self.epsilon, self.epsilon)
    else:
        normVal = torch.norm(delta.view(batchsize, -1), self.p, 1)
        mask = normVal<=self.epsilon
        scaling = self.epsilon/normVal
        scaling[mask] = 1
        delta = delta*scaling.view(batchsize, 1, 1, 1)
    advimage = image+delta
    
    advimage = torch.clamp(advimage, 0, 1)#cifar10(-1,1)
```

- DeepFool：一种通过线性逼近的方式寻找最小扰动量的攻击方法。



- CW（Carlini-Wagner）攻击：一种优化过程的攻击方法，通过优化模型输出和目标类之间的距离来生成对抗样本。

对抗攻击算法的研究是为了验证模型的鲁棒性、提高安全性，并推动对抗性训练等防御机制的发展。同时，对抗攻击算法也引发了对隐私问题的关注，因为攻击者可能通过对抗样本对原始数据进行推断或重建。因此，对抗攻击算法的研究与对抗性防御机制的研究同样重要。